# Copyright (c) Meta Platforms, Inc. and affiliates.
# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.

import json
import os
import sys
import time
from pathlib import Path
from typing import List, Literal, Optional, Tuple, TypedDict
import time

import torch
import torch.nn.functional as F

from fairscale.nn.model_parallel.initialize import (
    get_model_parallel_rank,
    initialize_model_parallel,
    model_parallel_is_initialized,
)

from llama.model import ModelArgs, Transformer
from llama.tokenizer import Tokenizer

from llama.model import change_pos, change_flag
import threading


# from example_chat_completion import len_predictor

Role = Literal["system", "user", "assistant"]


class Message(TypedDict):
    role: Role
    content: str


class CompletionPrediction(TypedDict, total=False):
    generation: str
    tokens: List[str]  # not required
    logprobs: List[float]  # not required


class ChatPrediction(TypedDict, total=False):
    generation: Message
    tokens: List[str]  # not required
    logprobs: List[float]  # not required


Dialog = List[Message]

B_INST, E_INST = "[INST]", "[/INST]"
B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
DEFAULT_SYSTEM_PROMPT = """\
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information."""

# DEFAULT_SYSTEM_PROMPT = """\
# Q: Statement 1 | Every element of a group generates a cyclic subgroup of the group. Statement 2 | The symmetric group S_10 has 10 elements.\n(A) True, True (B) False, False (C) True, False (D) False, True\nA: Let's think step by step. A cyclic group is a group that is generated by a single element. Hence a subgroup generated by a single element of a group is cyclic and Statement 1 is True. The answer is (C). You need to answer the following questions according to this model."""


SPECIAL_TAGS = [B_INST, E_INST, "<<SYS>>", "<</SYS>>"]
UNSAFE_ERROR = "Error: special tags are not allowed as part of the prompt."

class Llama:
    @staticmethod
    def build(
        ckpt_dir: str,
        tokenizer_path: str,
        max_seq_len: int,
        max_batch_size: int,
        model_parallel_size: Optional[int] = None,
    ) -> "Llama":
        if not torch.distributed.is_initialized():
            torch.distributed.init_process_group("nccl") # 设置后端，表明GPU之间的通信采用nccl方式
        if not model_parallel_is_initialized():
            if model_parallel_size is None:
                model_parallel_size = int(os.environ.get("WORLD_SIZE", 1))
                # print(model_parallel_size)
            initialize_model_parallel(model_parallel_size)

        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        torch.cuda.set_device(local_rank)

        # seed must be the same in all processes
        torch.manual_seed(1)

        if local_rank > 0:
            sys.stdout = open(os.devnull, "w")

        start_time = time.time()
        checkpoints = sorted(Path(ckpt_dir).glob("*.pth"))
        assert len(checkpoints) > 0, f"no checkpoint files found in {ckpt_dir}"
        assert model_parallel_size == len(
            checkpoints
        ), f"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}"
        ckpt_path = checkpoints[get_model_parallel_rank()]
        checkpoint = torch.load(ckpt_path, map_location="cuda")
        with open(Path(ckpt_dir) / "params.json", "r") as f:
            params = json.loads(f.read())

        model_args: ModelArgs = ModelArgs(
            max_seq_len=max_seq_len,
            max_batch_size=max_batch_size,
            **params,
        )
        tokenizer = Tokenizer(model_path=tokenizer_path)
        model_args.vocab_size = tokenizer.n_words
        torch.set_default_tensor_type(torch.cuda.HalfTensor) # half表示时16bit的浮点数
        # torch.set_default_tensor_type(torch.cuda.CharTensor) # half表示时16bit的浮点数
        model = Transformer(model_args)
        # torch.quantization.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False)
        model.load_state_dict(checkpoint, strict=False)
        # print(model)
        print(f"Loaded in {time.time() - start_time:.2f} seconds")

        return Llama(model, tokenizer)

    def __init__(self, model: Transformer, tokenizer: Tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.pred_len = None
        self.end = False

    def length_predict(self, bsz: int, len_predictor):
        assert(bsz == 32) # 默认会填充满最大的，所以只能是32
        t1 = time.time()
        print("thread running==============")

        while True:
            # print(self.model.layers[0].attention.latest_pos)
            pos = self.model.layers[0].attention.latest_pos
            if pos > 10:
                cache_k = self.model.layers[0].attention.cache_k[:bsz, :pos, :, :].float()
                input = cache_k.float()
                output = len_predictor(input)
                output = torch.argmax(output, dim=-1)  # (batchsize,) -> 0 short; 1 long
                print(output)
                assert(0) 

                self.pred_len = output1
                # break
                # if all(torch.all(output1 == 0)):
                #     break
                if self.end == True:
                    break
                t2 = time.time()
                # if (t2 - t1)  > 15:
                #     break


    @torch.inference_mode()
    def generate(
        self,
        len_predictor,
        prompt_tokens: List[List[int]],
        max_gen_len: int,
        temperature: float = 0.6,
        top_p: float = 0.6,
        logprobs: bool = False,
        echo: bool = False,
    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:

        # print(len_predictor.BATCH_SIZE)
        
        params = self.model.params
        bsz = len(prompt_tokens)
        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)

        # thread = threading.Thread(target = self.length_predict, args=(bsz,len_predictor,))
        # thread.start()

        # padding_tokens = prompt_tokens.copy()
        # for i in range(bsz):
        #     for j in range(len(padding_tokens[i]), 36):
        #         padding_tokens[i].append(0)
        #     if len(padding_tokens[i]) > 36:
        #         padding_tokens[i] = padding_tokens[i][:36]
        # print(len(padding_tokens[0]))
        # padding_tokens = torch.tensor(padding_tokens).cuda()
        # output = len_predictor(padding_tokens)
        # output = torch.argmax(output, dim=-1)  # (batchsize,) -> 0 short; 1 long
        # print(output)
        # assert(0)


        min_prompt_len = min(len(t) for t in prompt_tokens)
        max_prompt_len = max(len(t) for t in prompt_tokens)
        assert max_prompt_len <= params.max_seq_len
        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)

        pad_id = self.tokenizer.pad_id
        # 填充为size是[bsz, total_len]，value 为 pad_id的tensor，这里的bsz是1，total_len是512
        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device="cuda")
        for k, t in enumerate(prompt_tokens):
            # 相当于tokens的前面几个设置为prompt_token
            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device="cuda")
        if logprobs:
            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)

        prev_pos = 0

        eos_reached = torch.tensor([False] * bsz, device="cuda")
        input_text_mask = tokens != pad_id

        old_count = 0
        end_pos = []
        i = 0  # 上面三个变量用于统计结束时候的轮次
        flag = 1

        # tensor = torch.tensor([[552, 559, 6773, 1234, 2629, 29871, 29896,29900, 3838, 29889]])
        short_list = [[12148, 1234, 23359, 29889]]
        norm_list = [[12148, 1234, 6773, 29889]]

        start_time = time.time()
        if min_prompt_len == total_len:
            logits = self.model.forward(tokens, prev_pos, -1)
            token_logprobs = -F.cross_entropy(
                input=logits.transpose(1, 2),
                target=tokens,
                reduction="none",
                ignore_index=pad_id,
            )
        
        # print(prompt_tokens.shape)
        # print(min_prompt_len)
        for cur_pos in range(min_prompt_len, total_len):
            cache_cur_pos =  prev_pos if flag else -1
            
            # # if cur_pos - max_prompt_len == 80:
            # if cur_pos == 80:
            #     # t1 = time.time()
            #     cache_k = self.model.layers[31].attention.cache_k[:bsz, :cur_pos, :, :].float().cuda()
            #     cache_k = cache_k.view(bsz, cache_k.shape[1], -1).cuda()
            #     # input = torch.cat((cache_k, cache_v), dim=1).float()

            #     output = len_predictor(cache_k)
            #     output = torch.argmax(output, dim=-1)  # (batchsize,) -> 0 short; 1 long
            #     # print(output)
            #     # assert(0)
            #     all_same = len(set(output)) == 1
            #     list = []
            #     min_len = min(output)
            #     if not all_same:
            #         for i in range(bsz):
            #             if output[i] > min_len:
            #                 list += short_list
            #             else:
            #                 list += norm_list

            #         # for i in range(bsz):
            #         #     list += short_list

            #         tensor = torch.tensor(list).cuda()

            #         tensor_tmp = []
            #         for sentence_id in range(bsz):
            #             tensor_tmp.append([])
            #             if len(prompt_tokens[sentence_id]) >= cur_pos:
            #                 tensor_tmp[sentence_id] += prompt_tokens[sentence_id][:cur_pos - 4]
            #                 tensor_tmp[sentence_id] += (tensor[sentence_id].tolist())
            #                 tensor_tmp[sentence_id] += [518, 29914, 25580, 29962]
            #             else:
            #                 tensor_tmp[sentence_id] += prompt_tokens[sentence_id][:-4]
            #                 # for k in range(flag):
            #                 tensor_tmp[sentence_id] += (tensor[sentence_id].tolist())
            #                 tensor_tmp[sentence_id] += [518, 29914, 25580, 29962]
            #                 tensor_tmp[sentence_id] += (tokens[sentence_id, len(prompt_tokens[sentence_id]):cur_pos].tolist())
                        
            #         tensor_tmp = torch.tensor(tensor_tmp).cuda()
            #         # print(self.tokenizer.decode(tensor_tmp[:, :].tolist()))
                    
            #         logits = self.model.forward(tensor_tmp, 0, 1)
            #     else:
            #         logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos, -1)

            #     # flag += 1
            #     # self.end = True
            #     # t2 = time.time()
            # else:
            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos, -1)
            
            if logprobs:
                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(
                    input=logits.transpose(1, 2),
                    target=tokens[:, prev_pos + 1 : cur_pos + 1],
                    reduction="none",
                    ignore_index=pad_id,
                )
            if temperature > 0:
                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)
                next_token = sample_top_p(probs, top_p)
            else:
                next_token = torch.argmax(logits[:, -1], dim=-1)

            next_token = next_token.reshape(-1)
            next_token = torch.where(
                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token
            )
            tokens[:, cur_pos] = next_token
            eos_reached |= (~input_text_mask[:, cur_pos]) & (
                next_token == self.tokenizer.eos_id
            )
            prev_pos = cur_pos
            
            count = 0
            for j in range(len(eos_reached)):
                if(eos_reached[j] == True):
                    count += 1
            if count > old_count:
                old_count = count
                end_pos.append(i)
            
            i += 1
            if all(eos_reached):
                break

        if logprobs:
            token_logprobs = token_logprobs.tolist()
        out_tokens, out_logprobs = [], []
        for i, toks in enumerate(tokens.tolist()):
            # cut to max gen len
            start = 0 if echo else len(prompt_tokens[i]) # start 要么是0 要么是  prompt的长度
            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]
            probs = None
            if logprobs:
                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]
            # cut to eos tok if any
            if self.tokenizer.eos_id in toks:
                eos_idx = toks.index(self.tokenizer.eos_id)
                toks = toks[:eos_idx]
                probs = probs[:eos_idx] if logprobs else None
            out_tokens.append(toks)
            out_logprobs.append(probs)
        
        end_time = time.time()
        print("running time(ms) = ", (end_time - start_time) * 1000)
        with open("/home/snow/llama/llama/result/alpaca-llama-base-time.txt", "a") as file:
            file.write('time = %f' % ((end_time - start_time) * 1000))
            file.write('\n')
        return (out_tokens, out_logprobs if logprobs else None)

    def text_completion(
        self,
        prompts: List[str],
        temperature: float = 0.6,
        top_p: float = 0.9,
        max_gen_len: Optional[int] = None,
        logprobs: bool = False,
        echo: bool = False,
    ) -> List[CompletionPrediction]:
        if max_gen_len is None:
            max_gen_len = self.model.params.max_seq_len - 1
        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]
        # print(prompt_tokens)
        generation_tokens, generation_logprobs = self.generate(
            prompt_tokens=prompt_tokens,
            max_gen_len=max_gen_len,
            temperature=temperature,
            top_p=top_p,
            logprobs=logprobs,
            echo=echo,
        )
        # print(type(generation_tokens))

        if logprobs:
            return [
                {
                    "generation": self.tokenizer.decode(t),
                    "tokens": [self.tokenizer.decode(x) for x in t],
                    "logprobs": logprobs_i,
                }
                for t, logprobs_i in zip(generation_tokens, generation_logprobs)
            ]
        return [{"generation": self.tokenizer.decode(t)} for t in generation_tokens]

    def chat_completion(
        self,
        len_predictor,
        dialogs: List[Dialog],
        temperature: float = 0.6,
        top_p: float = 0.9,
        max_gen_len: Optional[int] = None,
        logprobs: bool = False,
    ) -> List[ChatPrediction]:
        if max_gen_len is None:
            max_gen_len = self.model.params.max_seq_len - 1 # 这个使用过命令行的输入，512
        prompt_tokens = []
        unsafe_requests = []
        for dialog in dialogs:
            unsafe_requests.append(
                any([tag in msg["content"] for tag in SPECIAL_TAGS for msg in dialog])
            )
            # if dialog[0]["role"] != "system":
            #     dialog = [
            #         {
            #             "role": "system",
            #             "content": DEFAULT_SYSTEM_PROMPT,
            #         }
            #     ] + dialog
            # dialog = [
            #     {
            #         "role": dialog[1]["role"],
            #         "content": B_SYS
            #         + dialog[0]["content"]
            #         + E_SYS
            #         + dialog[1]["content"],
            #     }
            # ] + dialog[2:]
            if dialog[0]["role"] == "system":
                dialog = [
                    {
                        "role": dialog[1]["role"],
                        "content": B_SYS
                        + dialog[0]["content"]
                        + E_SYS
                        + dialog[1]["content"],
                    }
                ] + dialog[2:]
            assert all([msg["role"] == "user" for msg in dialog[::2]]) and all(
                [msg["role"] == "assistant" for msg in dialog[1::2]]
            ), (
                "model only supports 'system', 'user' and 'assistant' roles, "
                "starting with 'system', then 'user' and alternating (u/a/u/a/u...)"
            )
            dialog_tokens: List[int] = sum(
                [
                    self.tokenizer.encode(
                        f"{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} ",
                        bos=True,
                        eos=True,
                    )
                    for prompt, answer in zip(
                        dialog[::2],
                        dialog[1::2],
                    )
                ],
                [],
            )
            # print(dialog_tokens)
            # print(dialog)
            # print(dialog[::2])
            assert (
                dialog[-1]["role"] == "user"
            ), f"Last message must be from user, got {dialog[-1]['role']}"
            dialog_tokens += self.tokenizer.encode(
                f"{B_INST} {(dialog[-1]['content']).strip()} {E_INST}",
                bos=True,
                eos=False,
            )
            # print(self.tokenizer.decode(dialog_tokens))
            prompt_tokens.append(dialog_tokens)

        # print(prompt_tokens) # 在这里开始变成了数字list，每一个list的长度不一致
        generation_tokens, generation_logprobs = self.generate(
            len_predictor=len_predictor,
            prompt_tokens=prompt_tokens,
            max_gen_len=max_gen_len,
            temperature=temperature,
            top_p=top_p,
            logprobs=logprobs,
        )
        if logprobs:
            return [
                {
                    "generation": {
                        "role": "assistant",
                        "content": self.tokenizer.decode(t)
                        if not unsafe
                        else UNSAFE_ERROR,
                    },
                    "tokens": [self.tokenizer.decode(x) for x in t],
                    "logprobs": logprobs_i,
                }
                for t, logprobs_i, unsafe in zip(
                    generation_tokens, generation_logprobs, unsafe_requests
                )
            ]
        return [
            {
                "generation": {
                    "role": "assistant",
                    "content": self.tokenizer.decode(t) if not unsafe else UNSAFE_ERROR,
                }
            }
            for t, unsafe in zip(generation_tokens, unsafe_requests)
        ]


def sample_top_p(probs, p):
    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort > p
    probs_sort[mask] = 0.0 # 对probs_sort张量进行按元素的掩码索引操作。
    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
    next_token = torch.multinomial(probs_sort, num_samples=1)
    next_token = torch.gather(probs_idx, -1, next_token) # 得到索引
    # print(next_token) # 返回索引
    return next_token
